{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpTqmwo5/SEYHgg8QZEp8J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trefftzc/cis677/blob/main/Introduction_to_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An introduction to PySpark\n",
        "\n",
        "Taken from https://sparkbyexamples.com/pyspark-tutorial/\n",
        "\n",
        "Installing:"
      ],
      "metadata": {
        "id": "BwPXNmAw4y9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WgIvDzM13lrk"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.5-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "import sys\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[2]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD Resilient Distributed Dataset\n",
        "\n",
        "PySpark RDD (Resilient Distributed Dataset) is a fundamental data structure that is fault-tolerant, immutable, and distributed collections of objects. RDDs are immutable, meaning they cannot be changed once created. Any transformation on an RDD results in a new RDD. Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster.\n",
        "\n",
        "RDD Creation\n",
        "In order to create an RDD, first, you need to create a SparkSession which is an entry point to the PySpark application. SparkSession can be created using a builder() or newSession() methods of the SparkSession.\n",
        "\n",
        "Spark session internally creates a sparkContext variable of SparkContext. You can create multiple SparkSession objects but only one SparkContext per JVM. In case you want to create another new SparkContext, you should stop the existing Sparkcontext (using stop()) before creating a new one.\n",
        "\n",
        "Let's create an RDD out of a list with 1024 entries"
      ],
      "metadata": {
        "id": "s9-ix1EZ57LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums = list(range(1, 1024))\n",
        "len(nums)\n",
        "\n",
        "nums_rdd = sc.parallelize(nums)"
      ],
      "metadata": {
        "id": "w5UuesDf6KcB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the first 5 entries:"
      ],
      "metadata": {
        "id": "1mSM7RDB6RT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums_rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3msP8i86VEB",
        "outputId": "5a119829-d202-482c-8bb3-a8f4c06ee8ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's solve the partition problem.\n",
        "Let's start by defining a function that will evaluate a possible partition:\n",
        "The array for the instance of the problem is hardcoded here."
      ],
      "metadata": {
        "id": "rqtVjqa960fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluatePartition(  value ) :\n",
        "   array = [1,1,1,1,1,1,1,1,1,9]\n",
        "   n = 10\n",
        "   sum0s = 0\n",
        "   sum1s = 0\n",
        "   mask = 1\n",
        "   for i in range(0,n):\n",
        "     if ((mask & value) != 0):\n",
        "       sum1s = sum1s + array[i]\n",
        "     else:\n",
        "       sum0s = sum0s + array[i]\n",
        "     mask = mask * 2\n",
        "\n",
        "   if (sum0s == sum1s):\n",
        "     return value\n",
        "   else:\n",
        "     return 0"
      ],
      "metadata": {
        "id": "B08iebHW6-ZR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluatePartition(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwBLdIqs7DUJ",
        "outputId": "058779e4-4cb5-46e9-b31c-c404fd774d0c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now apply the function evaluatePartition to the nums_rdd and store the results in a different rdd:"
      ],
      "metadata": {
        "id": "oq2FqhUq7H9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_rdd = nums_rdd.map(evaluatePartition)"
      ],
      "metadata": {
        "id": "ayY1twr67JMZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ltFKwSx7MjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, find if any entry turned to be different from 0, a solution"
      ],
      "metadata": {
        "id": "DU0QMBTh7OzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "sol = result_rdd.reduce(max)\n",
        "if sol == 0:\n",
        "  print(\"This instance of the partition problem does not have a solution\")\n",
        "else:\n",
        "  print(\"This instance of the partition problem does have a solution.\")\n",
        "print(sol)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZS_nX_W7P7R",
        "outputId": "8a8942db-c67c-476d-85fd-9aa7750f1d42"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This instance of the partition problem does have a solution.\n",
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Data Frame"
      ],
      "metadata": {
        "id": "FskbBn0H4moR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)"
      ],
      "metadata": {
        "id": "eafoy75V4o_x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kLIkRJGW4rE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since DataFrame is a tabular format that has names and data types in columns, use df.printSchema() to get the schema of the DataFrame.\n",
        "\n",
        "To display the DataFrame use df.show() which shows the 20 rows by default."
      ],
      "metadata": {
        "id": "QGscxqbV4w9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8iVvCAx47xq",
        "outputId": "b54af53a-8363-4136-d92c-615ea5bf68e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark SQL\n",
        "\n",
        "PySpark SQL is a module in Spark that provides a higher-level abstraction for working with structured data and can be used SQL queries.\n",
        "\n",
        "SQL enables you to write SQL queries against structured data, leveraging standard SQL syntax and semantics. This familiarity with SQL allows users with SQL proficiency to transition to Spark for data processing tasks easily.\n",
        "\n",
        "First, you should create a temporary table or view on DataFrame to use SQL queries. Once the table is created, you can be accessed throughout the SparkSession using sql().\n",
        "\n",
        "These tables and views are scoped to the SparkSession that created them. Once the SparkSession is terminated, either by closing the Spark application or ending the Spark session explicitly, the temporary views are removed from memory.\n",
        "\n",
        "To run SQL queries, use sql() method of the SparkSession object. Note that this function returns a DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "h-qp6H9N5FyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table\n",
        "df.createOrReplaceTempView(\"PERSON_DATA\")\n",
        "\n",
        "# Run SQL query\n",
        "df2 = spark.sql(\"SELECT * from PERSON_DATA\")\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMhBbQo55NCK",
        "outputId": "3d013c4c-8799-44f1-afb7-41cac7d02e1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use group by clause to run aggregate queries."
      ],
      "metadata": {
        "id": "R-phpcRo5Urp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using groupby\n",
        "groupDF = spark.sql(\"SELECT gender, count(*) from PERSON_DATA group by gender\")\n",
        "groupDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMGNmRRe5XDJ",
        "outputId": "bf7d0b60-d371-44af-89e1-c7f5becf85ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+\n",
            "|gender|count(1)|\n",
            "+------+--------+\n",
            "|     M|       3|\n",
            "|     F|       2|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning MLlib\n",
        "PySpark MLlib is Apache Spark’s scalable machine learning library, offering a suite of algorithms and tools for building, training, and deploying machine learning models. It provides implementations of popular algorithms for classification, regression, clustering, collaborative filtering, and more.\n",
        "\n",
        "MLlib is designed for distributed computing, allowing it to handle large-scale datasets across clusters efficiently. It offers both high-level APIs for ease of use and low-level APIs for fine-grained control over model training and evaluation.\n",
        "\n",
        "MLlib seamlessly integrates with Spark’s ecosystem, enabling end-to-end machine learning workflows, including data preprocessing, feature engineering, model training, and deployment in production environments.\n",
        "\n",
        "Here’s a simple example using Spark MLlib in Python to train a linear regression model:\n"
      ],
      "metadata": {
        "id": "eJEY6xKR5jpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Sample training data\n",
        "data = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0), (5.0, 6.0)]\n",
        "df = spark.createDataFrame(data, [\"features\", \"label\"])\n",
        "\n",
        "# Define a feature vector assembler\n",
        "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"features_vec\")\n",
        "\n",
        "# Transform the DataFrame with the feature vector assembler\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Create a LinearRegression model\n",
        "lr = LinearRegression(featuresCol=\"features_vec\", labelCol=\"label\")\n",
        "\n",
        "# Fit the model to the training data\n",
        "model = lr.fit(df)\n",
        "\n",
        "# Print the coefficients and intercept of the model\n",
        "print(\"Coefficients: \" + str(model.coefficients))\n",
        "print(\"Intercept: \" + str(model.intercept))\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ_dz-Y25rSa",
        "outputId": "f206cb61-447d-43ee-d7df-19b5a873da65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [1.0000000000000004]\n",
            "Intercept: 0.9999999999999986\n"
          ]
        }
      ]
    }
  ]
}