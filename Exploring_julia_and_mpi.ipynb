{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTJ1FUnSi+uJn1NCuGC7J+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trefftzc/cis677/blob/main/Exploring_julia_and_mpi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Julia"
      ],
      "metadata": {
        "id": "7IDoSJXiMoOq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSI81phCMUbU",
        "outputId": "ccc30ea8-8fa2-478f-8d00-f98f765842c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.11.0\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"MPI Statistics\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  nvidia-smi -L &> /dev/null && export GPU=1 || export GPU=0\n",
        "  if [ $GPU -eq 1 ]; then\n",
        "    JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia\n",
        "\n",
        "  echo ''\n",
        "  echo \"Successfully installed `julia -v`!\"\n",
        "  echo \"Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\"\n",
        "  echo \"jump to the 'Checking the Installation' section.\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A first program with Julia and MPI\n",
        "\n",
        "The following examples are taken from this web site:\n",
        "\n",
        "https://juliaparallel.org/MPI.jl/stable/examples/"
      ],
      "metadata": {
        "id": "SbaOqF37OKMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 01-hello.jl\n",
        "using MPI\n",
        "MPI.Init()\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "println(\"Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\")\n",
        "MPI.Barrier(comm)\n",
        "MPI.Finalize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O7Er7QAOM8I",
        "outputId": "e4c851cb-5fc2-4784-a484-006fd11ccc4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 01-hello.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQo-ONx8kfle",
        "outputId": "4a45362b-330d-4f9d-ff06-42c66cbe9c3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01-hello.jl  02-broadcast.jl  03-reduce.jl  04-sendrecv.jl  06-scatterv.jl  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install mpiexecjl to avoid problems with the version of MPI"
      ],
      "metadata": {
        "id": "u8DdN2yUQwsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!julia -e 'using MPI; MPI.install_mpiexecjl()'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgeS76YiPsdQ",
        "outputId": "ec798c01-f429-4bcb-e738-2c06581e1b8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91m\u001b[1mERROR: \u001b[22m\u001b[39mfile `/root/.julia/bin/mpiexecjl` already exists; use `MPI.install_mpiexecjl(force=true)` to overwrite.\n",
            "Stacktrace:\n",
            " [1] \u001b[0m\u001b[1merror\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90ms\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4merror.jl:35\u001b[24m\u001b[39m\n",
            " [2] \u001b[0m\u001b[1minstall_mpiexecjl\u001b[22m\u001b[0m\u001b[1m(\u001b[22m; \u001b[90mcommand\u001b[39m::\u001b[0mString, \u001b[90mdestdir\u001b[39m::\u001b[0mString, \u001b[90mforce\u001b[39m::\u001b[0mBool, \u001b[90mverbose\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[90m   @\u001b[39m \u001b[35mMPI\u001b[39m \u001b[90m~/.julia/packages/MPI/TKXAj/src/\u001b[39m\u001b[90m\u001b[4mmpiexec_wrapper.jl:18\u001b[24m\u001b[39m\n",
            " [3] \u001b[0m\u001b[1minstall_mpiexecjl\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
            "\u001b[90m   @\u001b[39m \u001b[35mMPI\u001b[39m \u001b[90m~/.julia/packages/MPI/TKXAj/src/\u001b[39m\u001b[90m\u001b[4mmpiexec_wrapper.jl:11\u001b[24m\u001b[39m\n",
            " [4] top-level scope\n",
            "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mnone:1\u001b[24m\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "/root/.julia/bin/mpiexecjl --project=/content -n 3 julia 01-hello.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBqLmjOaOUrA",
        "outputId": "d8fcea0f-50c0-4871-9e59-7a9cd29b93e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world, I am 1 of 3\n",
            "Hello world, I am 2 of 3\n",
            "Hello world, I am 0 of 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An example of Broadcast"
      ],
      "metadata": {
        "id": "6vMEI5TuToTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 02-broadcast.jl\n",
        "import MPI\n",
        "MPI.Init()\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "N = 5\n",
        "root = 0\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    print(\" Running on $(MPI.Comm_size(comm)) processes\\n\")\n",
        "end\n",
        "MPI.Barrier(comm)\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    A = [i*(1.0 + im*2.0) for i = 1:N]\n",
        "else\n",
        "    A = Array{ComplexF64}(undef, N)\n",
        "end\n",
        "\n",
        "MPI.Bcast!(A, root, comm)\n",
        "\n",
        "print(\"rank = $(MPI.Comm_rank(comm)), A = $A\\n\")\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    B = Dict(\"foo\" => \"bar\")\n",
        "else\n",
        "    B = nothing\n",
        "end\n",
        "\n",
        "B = MPI.bcast(B, root, comm)\n",
        "print(\"rank = $(MPI.Comm_rank(comm)), B = $B\\n\")\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    f = x -> x^2 + 2x - 1\n",
        "else\n",
        "    f = nothing\n",
        "end\n",
        "\n",
        "f = MPI.bcast(f, root, comm)\n",
        "print(\"rank = $(MPI.Comm_rank(comm)), f(3) = $(f(3))\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWJvZmEIQV2Q",
        "outputId": "3ab2ee6a-fcc0-4013-b5e1-3c46467b3859"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 02-broadcast.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now executing the code"
      ],
      "metadata": {
        "id": "JwCEcZ07TwwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/root/.julia/bin/mpiexecjl --project=/content -n 4 julia 02-broadcast.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDSuft2oTys3",
        "outputId": "d2fa185e-4a64-47f1-b288-c7f37895349b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Running on 4 processes\n",
            "rank = 0, A = ComplexF64[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\n",
            "rank = 3, A = ComplexF64[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\n",
            "rank = 1, A = ComplexF64[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\n",
            "rank = 2, A = ComplexF64[1.0 + 2.0im, 2.0 + 4.0im, 3.0 + 6.0im, 4.0 + 8.0im, 5.0 + 10.0im]\n",
            "rank = 0, B = Dict(\"foo\" => \"bar\")\n",
            "rank = 2, B = Dict(\"foo\" => \"bar\")\n",
            "rank = 1, B = Dict(\"foo\" => \"bar\")\n",
            "rank = 3, B = Dict(\"foo\" => \"bar\")\n",
            "rank = 0, f(3) = 14\n",
            "rank = 3, f(3) = 14\n",
            "rank = 1, f(3) = 14\n",
            "rank = 2, f(3) = 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now an example of reduce"
      ],
      "metadata": {
        "id": "6uw5NN1uUHof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 03-reduce.jl\n",
        "# This example shows how to use custom datatypes and reduction operators\n",
        "# It computes the variance in parallel in a numerically stable way\n",
        "\n",
        "using MPI, Statistics\n",
        "\n",
        "MPI.Init()\n",
        "const comm = MPI.COMM_WORLD\n",
        "const root = 0\n",
        "\n",
        "# Define a custom struct\n",
        "# This contains the summary statistics (mean, variance, length) of a vector\n",
        "struct SummaryStat\n",
        "    mean::Float64\n",
        "    var::Float64\n",
        "    n::Float64\n",
        "end\n",
        "function SummaryStat(X::AbstractArray)\n",
        "    m = mean(X)\n",
        "    v = varm(X,m, corrected=false)\n",
        "    n = length(X)\n",
        "    SummaryStat(m,v,n)\n",
        "end\n",
        "\n",
        "# Define a custom reduction operator\n",
        "# this computes the pooled mean, pooled variance and total length\n",
        "function pool(S1::SummaryStat, S2::SummaryStat)\n",
        "    n = S1.n + S2.n\n",
        "    m = (S1.mean*S1.n + S2.mean*S2.n) / n\n",
        "    v = (S1.n * (S1.var + S1.mean * (S1.mean-m)) +\n",
        "         S2.n * (S2.var + S2.mean * (S2.mean-m)))/n\n",
        "    SummaryStat(m,v,n)\n",
        "end\n",
        "\n",
        "# Register the custom reduction operator.  This is necessary only on platforms\n",
        "# where Julia doesn't support closures as cfunctions (e.g. ARM), but can be used\n",
        "# on all platforms for consistency.\n",
        "MPI.@RegisterOp(pool, SummaryStat)\n",
        "\n",
        "X = randn(10,3) .* [1,3,7]'\n",
        "\n",
        "# Perform a scalar reduction\n",
        "summ = MPI.Reduce(SummaryStat(X), pool, comm; root)\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    @show summ.var\n",
        "end\n",
        "\n",
        "# Perform a vector reduction:\n",
        "# the reduction operator is applied elementwise\n",
        "col_summ = MPI.Reduce(mapslices(SummaryStat,X,dims=1), pool, comm; root)\n",
        "\n",
        "if MPI.Comm_rank(comm) == root\n",
        "    col_var = map(summ -> summ.var, col_summ)\n",
        "    @show col_var\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVQro-N0UGLw",
        "outputId": "f2380589-1bf6-4e45-e4fb-49b54cec4b2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 03-reduce.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now execute the reduction example"
      ],
      "metadata": {
        "id": "7XCBGrWQUrU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/root/.julia/bin/mpiexecjl --project=/content -n 4 julia 03-reduce.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UbnkgU7Ug1g",
        "outputId": "28b9ebb4-db2c-4c94-d192-965152079038"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summ.var = 18.885008706354128\n",
            "col_var = [0.9622197336623035 8.45653007750693 45.222046446282]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of send and receive"
      ],
      "metadata": {
        "id": "AvOBq4VUUx63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 04-sendrecv.jl\n",
        "using MPI\n",
        "MPI.Init()\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = MPI.Comm_rank(comm)\n",
        "size = MPI.Comm_size(comm)\n",
        "\n",
        "dst = mod(rank+1, size)\n",
        "src = mod(rank-1, size)\n",
        "\n",
        "N = 4\n",
        "\n",
        "send_mesg = Array{Float64}(undef, N)\n",
        "recv_mesg = Array{Float64}(undef, N)\n",
        "\n",
        "fill!(send_mesg, Float64(rank))\n",
        "\n",
        "rreq = MPI.Irecv!(recv_mesg, comm; source=src, tag=src+32)\n",
        "\n",
        "print(\"$rank: Sending   $rank -> $dst = $send_mesg\\n\")\n",
        "sreq = MPI.Isend(send_mesg, comm; dest=dst, tag=rank+32)\n",
        "\n",
        "stats = MPI.Waitall([rreq, sreq])\n",
        "\n",
        "print(\"$rank: Received $src -> $rank = $recv_mesg\\n\")\n",
        "\n",
        "MPI.Barrier(comm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPSOiIyiU0VH",
        "outputId": "f0b0cfb8-1f41-4a18-f2fe-dbd743b72db2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 04-sendrecv.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now its execution:"
      ],
      "metadata": {
        "id": "a806djlqU67v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/root/.julia/bin/mpiexecjl --project=/content -n 4 julia 04-sendrecv.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oH6g_6oU-j3",
        "outputId": "aaa14a69-5890-4ae4-bc7b-3dac697e8faa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: Sending   1 -> 2 = [1.0, 1.0, 1.0, 1.0]\n",
            "3: Sending   3 -> 0 = [3.0, 3.0, 3.0, 3.0]\n",
            "0: Sending   0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n",
            "2: Sending   2 -> 3 = [2.0, 2.0, 2.0, 2.0]\n",
            "1: Received 0 -> 1 = [0.0, 0.0, 0.0, 0.0]\n",
            "0: Received 3 -> 0 = [3.0, 3.0, 3.0, 3.0]\n",
            "3: Received 2 -> 3 = [2.0, 2.0, 2.0, 2.0]\n",
            "2: Received 1 -> 2 = [1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of how to use scatter and gather"
      ],
      "metadata": {
        "id": "BbDYLsSFVSHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 06-scatterv.jl\n",
        "# This example shows how to use MPI.Scatterv! and MPI.Gatherv!\n",
        "# roughly based on the example from\n",
        "# https://stackoverflow.com/a/36082684/392585\n",
        "\n",
        "using MPI\n",
        "\n",
        "\"\"\"\n",
        "    split_count(N::Integer, n::Integer)\n",
        "\n",
        "Return a vector of `n` integers which are approximately equally sized and sum to `N`.\n",
        "\"\"\"\n",
        "function split_count(N::Integer, n::Integer)\n",
        "    q,r = divrem(N, n)\n",
        "    return [i <= r ? q+1 : q for i = 1:n]\n",
        "end\n",
        "\n",
        "\n",
        "MPI.Init()\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = MPI.Comm_rank(comm)\n",
        "comm_size = MPI.Comm_size(comm)\n",
        "\n",
        "root = 0\n",
        "\n",
        "if rank == root\n",
        "    M, N = 4, 7\n",
        "\n",
        "    test = Float64[i for i = 1:M, j = 1:N]\n",
        "    output = similar(test)\n",
        "\n",
        "    # Julia arrays are stored in column-major order, so we need to split along the last dimension\n",
        "    # dimension\n",
        "    M_counts = [M for i = 1:comm_size]\n",
        "    N_counts = split_count(N, comm_size)\n",
        "\n",
        "    # store sizes in 2 * comm_size Array\n",
        "    sizes = vcat(M_counts', N_counts')\n",
        "    size_ubuf = UBuffer(sizes, 2)\n",
        "\n",
        "    # store number of values to send to each rank in comm_size length Vector\n",
        "    counts = vec(prod(sizes, dims=1))\n",
        "\n",
        "    test_vbuf = VBuffer(test, counts) # VBuffer for scatter\n",
        "    output_vbuf = VBuffer(output, counts) # VBuffer for gather\n",
        "else\n",
        "    # these variables can be set to `nothing` on non-root processes\n",
        "    size_ubuf = UBuffer(nothing)\n",
        "    output_vbuf = test_vbuf = VBuffer(nothing)\n",
        "end\n",
        "\n",
        "if rank == root\n",
        "    println(\"Original matrix\")\n",
        "    println(\"================\")\n",
        "    @show test sizes counts\n",
        "    println()\n",
        "    println(\"Each rank\")\n",
        "    println(\"================\")\n",
        "end\n",
        "MPI.Barrier(comm)\n",
        "\n",
        "local_size = MPI.Scatter(size_ubuf, NTuple{2,Int}, root, comm)\n",
        "local_test = MPI.Scatterv!(test_vbuf, zeros(Float64, local_size), root, comm)\n",
        "\n",
        "for i = 0:comm_size-1\n",
        "    if rank == i\n",
        "        @show rank local_test\n",
        "    end\n",
        "    MPI.Barrier(comm)\n",
        "end\n",
        "\n",
        "MPI.Gatherv!(local_test, output_vbuf, root, comm)\n",
        "\n",
        "if rank == root\n",
        "    println()\n",
        "    println(\"Final matrix\")\n",
        "    println(\"================\")\n",
        "    @show output\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfMLckJfVWmH",
        "outputId": "9bbb7c2d-d557-4928-fb0f-1c091e940314"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 06-scatterv.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/root/.julia/bin/mpiexecjl --project=/content -n 4 julia 06-scatterv.jl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoudMhtUVd2v",
        "outputId": "ce6c61b0-c093-4143-fed3-55bdcb645ab7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original matrix\n",
            "================\n",
            "test = [1.0 1.0 1.0 1.0 1.0 1.0 1.0; 2.0 2.0 2.0 2.0 2.0 2.0 2.0; 3.0 3.0 3.0 3.0 3.0 3.0 3.0; 4.0 4.0 4.0 4.0 4.0 4.0 4.0]\n",
            "sizes = [4 4 4 4; 2 2 2 1]\n",
            "counts = [8, 8, 8, 4]\n",
            "\n",
            "Each rank\n",
            "================\n",
            "rank = 0\n",
            "local_test = [1.0 1.0; 2.0 2.0; 3.0 3.0; 4.0 4.0]\n",
            "rank = 1\n",
            "local_test = [1.0 1.0; 2.0 2.0; 3.0 3.0; 4.0 4.0]\n",
            "rank = 2\n",
            "local_test = [1.0 1.0; 2.0 2.0; 3.0 3.0; 4.0 4.0]\n",
            "rank = 3\n",
            "local_test = [1.0; 2.0; 3.0; 4.0;;]\n",
            "\n",
            "Final matrix\n",
            "================\n",
            "output = [1.0 1.0 1.0 1.0 1.0 1.0 1.0; 2.0 2.0 2.0 2.0 2.0 2.0 2.0; 3.0 3.0 3.0 3.0 3.0 3.0 3.0; 4.0 4.0 4.0 4.0 4.0 4.0 4.0]\n"
          ]
        }
      ]
    }
  ]
}